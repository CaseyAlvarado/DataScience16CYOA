{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotten Tomatoes Sentiment Prediction\n",
    "This notebook describes the models that Anne LoVerso and Casey Alvarado used to try our hand at the [Kaggle Movie Review Sentiment](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews) problem.  To summarize, the Kaggle data gives us a series of sentences and their corresponding sentiment value on the following scale:\n",
    "- 0: very negative\n",
    "- 1: somewhat negative\n",
    "- 2: neutral\n",
    "- 3: somewhat positive\n",
    "- 4: very positive\n",
    "\n",
    "The sentences are also broken down into chunks of phrases that make up each sentence, and the sentiment of each phrase that makes up a sentence.\n",
    "\n",
    "## Exploration\n",
    "- pattern sentiment (but minimize)\n",
    "- subj/obj and phrase length\n",
    "- modality and mood (summarize all the pattern stuff bc we didn't really use it)\n",
    "- figure out what word2vec does - if it's similar to nltk tagging then include that\n",
    "\n",
    "- sentence sentiment vs average sentiment of its phrases\n",
    "\n",
    "- WORD CLOUD\n",
    "- include most pos/neg words (relate to naive bayes + flow into scikit learn)\n",
    "\n",
    "## Classification Models\n",
    "- copy the model-iterations notebook but clean the code to make sure it isn't repetitive\n",
    "\n",
    "## Neural Networks\n",
    "- Code from lasagne and results\n",
    "- add lots of comments to show we know what it does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro Code\n",
    "This code that creates the data and testdata pandas dataframe, and a function that makes a Kaggle submission file, gets used consistently throughout the file, so both of these should be run at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas \n",
    "data = pandas.read_csv('train.tsv', sep = '\\t') \n",
    "testdata = pandas.read_csv('test.tsv', sep = '\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predictions, filename):\n",
    "    # Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "    submission = pandas.DataFrame({\n",
    "            \"PhraseId\": testdata[\"PhraseId\"],\n",
    "            \"Sentiment\": predictions\n",
    "        })\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models\n",
    "This section describes Anne and Casey's first attempt at building models for [Kaggle Movie Review Sentiment](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews).  We decided that for a first iteration, we would try a lot of different models based on research that we had done, learning as much as we could while seeing what would work the best.\n",
    "\n",
    "The sections are:\n",
    "- Naive Bayes Classifier\n",
    "    - implemented by hand\n",
    "    - using sklearn MultinomialNB\n",
    "    - using NLTK\n",
    "- sklearn Pipeline with a variety of estimators\n",
    "- Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with Lasagne\n",
    "This section describes our work with implementing a neural network in using the  [Lasagne](https://github.com/Lasagne/Lasagne) package.  Our code for this is heavily based off the [Lasagne MNIST tutorial](http://lasagne.readthedocs.org/en/latest/user/tutorial.html) and we used their method of implementing a Multi-Layer Perceptron model and training it on our particular dataset.\n",
    "\n",
    "## Word2Vec\n",
    "We loaded data into our network using word2vec encodings of our original phrase data.  The word2vec algorithm by Google used billions of Google news data sources to train a model that basically provides a lookup table for an enormous vocabulary.  The idea is that for the words, it would be able to provide a 300-dimensional vector of features that represents the various semantic properties of a word.  This vector could then be used to predict similar words or words that are likely to succeed other words.\n",
    "\n",
    "In using word2vec in our encodings, we are able to use it as a lookup table.  Our training data has the shape (68030, 300) because we have 68030 phrases.  For each phrase, word2vec was used on each word to generate a 300-dimensional vector of features for the word.  Then, the vectors for all words in a phrase were averaged together to yield a single 1x300 vector for a phrase that represented the average semantic encoding of the phrase.  This was done for all 68030 phrases to give us the final dense matrix of 68030 rows and 300 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import pandas \n",
    "import pickle\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import LabelShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    # load the training data using pickle\n",
    "    # the data X has shape (68030, 300) and is the average word2vec semantic encodings for each phrase\n",
    "    # the data y has shape (68030,) and is the sentiment for each phrase from 0-4\n",
    "    f = open('rotten_tomatoes_train.pickle')\n",
    "    X = pickle.load(f)\n",
    "    y = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    def make_splits(X,y):\n",
    "        splits = LabelShuffleSplit(data.SentenceId, test_size=0.5, n_iter=1)\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train = X[train_indices,:]\n",
    "            y_train = y[train_indices]\n",
    "            X_test = X[test_indices, :]\n",
    "            y_test = y[test_indices]\n",
    "            return X_train, X_test, y_train, y_test\n",
    "\n",
    "    # make train_test_split on the sentenceID\n",
    "    X_train, X_test, y_train, y_test = make_splits(X,y)\n",
    "    \n",
    "    # we used sklearn's train_test_split to separate 50% of our data as test data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # we return all the arrays (in np.asarray format, so that it is compatible with theano)\n",
    "    return np.asarray(X_train), np.asarray(y_train), np.asarray(X_val), np.asarray(y_val), np.asarray(X_test), np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLP\n",
    "The following function builds a Multi-Layer Perceptron model for our neural network.  Based on our research, an MLP as we understand it is a way to expand upon Logistic Regression by adding one or more hidden layers.  The input layer maps to the hidden layer through a set of weights that are learned, and then it maps from the final hidden layer to a set of outputs through another set of learned weights.\n",
    "\n",
    "Our MLP has two hidden layers of 800 units each.  It applies 50% dropout following each hidden layer and 20% dropout to the initial input.  It ends up with a softmax output layer with 5 units, for the 5 classes of sentiment in our data.  The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) is a form of normalized logistic regression (that is necessary in neural networks) but that is commonly used in multiclass classification such as this problem.  The specifications of the network and each layer are based on the Lasagne tutorial.\n",
    "\n",
    "A representation of the layers of our MLP network is:\n",
    "\n",
    "Input | 20% dropout | Hidden layer 1 | 50% dropout | Hidden layer 2 | 50% dropout | Output layer (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 5 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 300),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # 50% dropout again:\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer, of 5 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2_drop, num_units=5,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Iterator\n",
    "This section (again, using code from teh Lasagne tutorial) creates a helper function that, using yield, returns an iterable generator object that allows us to create batches from our full input data (either randomized or sequential depending on the shuffle variable).  This function gets used in the training of our model and the train function that we define is performed on each minibatch, and the loss for each epoch is the sum of the loss for each batch within the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == len(targets)\n",
    "    print (inputs.shape[0])\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training\n",
    "The training of the model uses 500 epochs and trains in minibatches on each epoch.\n",
    "\n",
    "Before training, we create the loss and update functions.  The update function uses Stochastic Gradient Descent.  The error for each phrase and sentiment value can be calculated, and then we need the gradient of the error, and we want to sum all these gradients for all data points.  This is computationally expensive so instead it chooses a random x,y pair (in our case, a phrase and corresponding sentiment value) to calculate the gradient and moves along the curve relative to the error of the previous point so that it can move more finely when necessary.  Lasagne implements this algorithm for us.\n",
    "\n",
    "The loss function uses [categorical cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression), which calculates a measure of similarity between the prediction for a given data point and its actual value.  This is one of many loss functions that Lasagne provides, and we chose to use it based on the tutorial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define number of epochs to train for\n",
    "num_epochs=500\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "# input_var is a matrix because it will be 2-dimensional (68030,300)\n",
    "# and target_var is a vector because it will be 1-dimensional (68030,)\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_mlp(input_var)\n",
    "\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "# this function will be used to predict on our actual test data to make a Kaggle submission\n",
    "pred_fn = theano.function([input_var], test_prediction)\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        a = train_fn(inputs, targets) #a is the loss returned by train_fn\n",
    "        train_err += a\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "# dump the network weights to a file like this:\n",
    "np.savez('model3.npz', *lasagne.layers.get_all_param_values(network))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "This code uses the trained network to make predictions and create a Kaggle submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make predictions and submission\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "network = build_mlp(input_var)\n",
    "\n",
    "# load the model\n",
    "with np.load('model2.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "\n",
    "pred_fn = theano.function([input_var], test_prediction)\n",
    "\n",
    "# load the test data\n",
    "f = open('X_test_word2vec.pickle')\n",
    "X_testdata = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# make predictions!\n",
    "pred_probabilities = pred_fn(X_testdata)\n",
    "\n",
    "predictions = list(map(lambda pr: list(pr).index(max(pr)), pred_probabilities))\n",
    "\n",
    "make_submission(predictions, \"neural2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
